{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\shubh\\\\Desktop\\\\Important_IPYNB_files\\\\NLP\\Other_files\\\\IMDB_Dataset.csv\"\n",
    "df = pd.read_csv(path)\n",
    "df = df[:10000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A wonderful little production. <br /><br />The filming technique is very unassuming- very old-time-BBC fashion and gives a comforting, and sometimes discomforting, sense of realism to the entire piece. <br /><br />The actors are extremely well chosen- Michael Sheen not only \"has got all the polari\" but he has all the voices down pat too! You can truly see the seamless editing guided by the references to Williams\\' diary entries, not only is it well worth the watching but it is a terrificly written and performed piece. A masterful production about one of the great master\\'s of comedy and his life. <br /><br />The realism really comes home with the little things: the fantasy of the guard which, rather than use the traditional \\'dream\\' techniques remains solid then disappears. It plays on our knowledge and our senses, particularly with the scenes concerning Orton and Halliwell and the sets (particularly of their flat with Halliwell\\'s murals decorating every surface) are terribly well done.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "positive    5028\n",
       "negative    4972\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking class balance\n",
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "review       0\n",
       "sentiment    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NULL values\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicate values\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Basic preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "import re\n",
    "\n",
    "def remove_tags(raw_text):\n",
    "    cleaned_text = re.sub(re.compile('<.*?>'), '', raw_text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['review'] = df['review'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing\n",
    "df['review'] = df['review'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_list = stopwords.words('english')\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wonderful little production. filming technique unassuming- old-time-bbc fashion gives comforting, sometimes discomforting, sense realism entire piece. actors extremely well chosen- michael sheen \"has got polari\" voices pat too! truly see seamless editing guided references williams\\' diary entries, well worth watching terrificly written performed piece. masterful production one great master\\'s comedy life. realism really comes home little things: fantasy guard which, rather use traditional \\'dream\\' techniques remains solid disappears. plays knowledge senses, particularly scenes concerning orton halliwell sets (particularly flat halliwell\\'s murals decorating every surface) terribly well done.'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['review'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperating X(input data) and y(class variable)\n",
    "X = df.iloc[:,0:1]\n",
    "y = df['sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting y(class variable) into 0 & 1\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y = encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train-Test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (7986, 1)\n",
      "Test data: (1997, 1)\n"
     ]
    }
   ],
   "source": [
    "print('Train data:', X_train.shape)\n",
    "print('Test data:', X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Text Vectorization(Feature Engineering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.1. BOW & Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data: (7986, 48282)\n",
      "Test data: (1997, 48282)\n"
     ]
    }
   ],
   "source": [
    "# Applying BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "print('Train data:', X_train_bow.shape)\n",
    "print('Test data:', X_test_bow.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6324486730095142\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[717, 235],\n",
       "       [499, 546]], dtype=int64)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Applying Naive Bayes\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train_bow,y_train)\n",
    "y_pred = gnb.predict(X_test_bow)\n",
    "\n",
    "print(accuracy_score(y_test,y_pred))\n",
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2. BOW & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8507761642463696\n"
     ]
    }
   ],
   "source": [
    "# Applying Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "print(accuracy_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8372558838257386"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BOW: Most frequently used 3000 features(words in our case)\n",
    "cv = CountVectorizer(max_features=3000)\n",
    "\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.3. N-grams & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8462694041061593"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer(ngram_range=(1,2),max_features=5000)\n",
    "\n",
    "X_train_bow = cv.fit_transform(X_train['review']).toarray()\n",
    "X_test_bow = cv.transform(X_test['review']).toarray()\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_bow,y_train)\n",
    "y_pred = rf.predict(X_test_bow)\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.4. TF-IDF & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8437656484727091"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "\n",
    "X_train_tfidf = tfidf.fit_transform(X_train['review']).toarray()\n",
    "X_test_tfidf = tfidf.transform(X_test['review'])\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "rf.fit(X_train_tfidf,y_train)\n",
    "y_pred = rf.predict(X_test_tfidf)\n",
    "\n",
    "accuracy_score(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.5. Custom W2V(on our own data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"C:\\\\Users\\\\shubh\\\\Desktop\\\\Important_IPYNB_files\\\\NLP\\Other_files\\\\IMDB_Dataset.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates\n",
    "df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove HTML tags\n",
    "import re\n",
    "\n",
    "def remove_tags(raw_text):\n",
    "    cleaned_text = re.sub(re.compile('<.*?>'), '', raw_text)\n",
    "    return cleaned_text\n",
    "\n",
    "df['review'] = df['review'].apply(remove_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lower casing\n",
    "df['review'] = df['review'].apply(lambda x:x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "sw_list = stopwords.words('english')\n",
    "\n",
    "df['review'] = df['review'].apply(lambda x: [item for item in x.split() if item not in sw_list]).apply(lambda x:\" \".join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29540314, 30891345)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from nltk import sent_tokenize\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "story = []\n",
    "\n",
    "# Sentence Tokenization\n",
    "for doc in df['review']:\n",
    "    raw_sent = sent_tokenize(doc)\n",
    "    for sent in raw_sent:\n",
    "        story.append(simple_preprocess(sent))\n",
    "\n",
    "# Initializing W2V model\n",
    "model = gensim.models.Word2Vec(\n",
    "    window=10,         # window=10 means ---> 10 words on either side of our focus/target word.\n",
    "    min_count=2,       # picking sentences with atleast 2 words\n",
    "    workers=4          # 4 cores parallely\n",
    ")\n",
    "\n",
    "# building vocabulary from the list 'story'\n",
    "model.build_vocab(story)\n",
    "\n",
    "# Training w2v on our own data\n",
    "model.train(story, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary: 61843\n"
     ]
    }
   ],
   "source": [
    "# Length of Vocabulary\n",
    "print('Length of Vocabulary:', len(model.wv.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.6. Average W2V(on our own data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.10501339,  0.30747497,  0.17245911, -0.13408558,  0.25844842,\n",
       "       -0.02025337, -0.20083608,  0.61111915,  0.40674952, -0.15839246,\n",
       "        0.1229958 , -0.3683737 ,  0.44359496,  0.06434723, -0.07597051,\n",
       "        0.0394401 , -0.15392599, -0.0322172 , -0.10481744, -0.16380973,\n",
       "       -0.24960642,  0.2344628 , -0.01237789,  0.12628049, -0.03454323,\n",
       "       -0.0473893 ,  0.26033342,  0.12324991, -0.37490442, -0.18179458,\n",
       "        0.3657113 ,  0.05142675,  0.19240575, -0.3283031 , -0.3687984 ,\n",
       "        0.23345576, -0.10905088, -0.11924558,  0.07118194, -0.5039484 ,\n",
       "       -0.07678928, -0.26330823, -0.2491042 , -0.32948092,  0.4081078 ,\n",
       "       -0.11376476, -0.26003194, -0.05677945,  0.23286341,  0.15037318,\n",
       "        0.29430714, -0.04465853, -0.21874185, -0.47279727, -0.14038189,\n",
       "        0.16437168,  0.1698023 , -0.178364  , -0.11313047, -0.07902962,\n",
       "        0.24557208,  0.07740561, -0.2806842 ,  0.09889531, -0.03227866,\n",
       "       -0.02838442, -0.41858244, -0.33597028, -0.21707219, -0.27681127,\n",
       "       -0.04219418, -0.13966304,  0.17070891,  0.09650253,  0.36410785,\n",
       "       -0.7011091 ,  0.2980338 , -0.42886066, -0.6669642 ,  0.19731523,\n",
       "       -0.32519132, -0.37910163, -0.1399471 , -0.03829061, -0.16892904,\n",
       "       -0.2733122 ,  0.3813188 , -0.16126288,  0.21351443,  0.19410421,\n",
       "        0.26902485, -0.49562228,  0.5045182 ,  0.09253638,  0.47493386,\n",
       "        0.41154715,  0.2766982 , -0.08127119,  0.06999487,  0.2105496 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting entire document(review) into a numerical vector\n",
    "def document_vector(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc.split() if word in model.wv.index_to_key]\n",
    "    return np.mean(model.wv[doc], axis=0)\n",
    "\n",
    "# This is the first review\n",
    "document_vector(df['review'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 10000/10000 [19:27<00:00,  8.57it/s]\n"
     ]
    }
   ],
   "source": [
    "# Performing the above transformation for entire corpus\n",
    "\n",
    "df = df[:10000]\n",
    "from tqdm import tqdm\n",
    "\n",
    "X = []\n",
    "\n",
    "for doc in tqdm(df['review'].values):\n",
    "    X.append(document_vector(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.10501339  0.30747497  0.17245911 ... -0.08127119  0.06999487\n",
      "   0.2105496 ]\n",
      " [ 0.19889541  0.15217632  0.05717173 ...  0.3055233  -0.1265673\n",
      "   0.13415943]\n",
      " [ 0.24245839  0.18394384 -0.1782204  ... -0.29714575  0.05394321\n",
      "   0.09020282]\n",
      " ...\n",
      " [ 0.536764    0.17784327  0.32414502 ...  0.07521009  0.16796497\n",
      "   0.20907971]\n",
      " [-0.07166822  0.42849728  0.00441418 ...  0.04540765  0.09674265\n",
      "  -0.25315028]\n",
      " [ 0.17054656  0.24721685  0.18980955 ...  0.00898315 -0.02774275\n",
      "  -0.12952904]]\n",
      "<class 'numpy.ndarray'>\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "print(X)\n",
    "print(type(X))\n",
    "print(len(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X to a numpy array\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 100)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.1. Average W2V & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>one reviewers mentioned watching 1 oz episode ...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>wonderful little production. filming technique...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>basically there's family little boy (jake) thi...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>petter mattei's \"love time money\" visually stu...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review sentiment\n",
       "0  one reviewers mentioned watching 1 oz episode ...  positive\n",
       "1  wonderful little production. filming technique...  positive\n",
       "2  thought wonderful way spend time hot summer we...  positive\n",
       "3  basically there's family little boy (jake) thi...  negative\n",
       "4  petter mattei's \"love time money\" visually stu...  positive"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 0])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting target variable(sentiment) into binary\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "\n",
    "y = encoder.fit_transform(df['sentiment'])\n",
    "\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.829"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 6.2. W2V(Google) & Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec,KeyedVectors\n",
    "from gensim.utils import simple_preprocess\n",
    "from nltk import sent_tokenize\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Vocabulary: 3000000\n"
     ]
    }
   ],
   "source": [
    "# Length of Vocabulary\n",
    "print('Length of Vocabulary:', len(model.index_to_key))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 3.56353521e-02,  5.20200655e-02,  3.68175954e-02,  8.18563327e-02,\n",
       "       -5.47294319e-02,  2.73479684e-03,  5.26788309e-02, -7.66736791e-02,\n",
       "        7.87198469e-02,  9.91729796e-02,  6.10370189e-03, -1.19252957e-01,\n",
       "       -2.28140596e-03,  3.49528007e-02, -1.02423519e-01,  7.97183663e-02,\n",
       "        3.08098514e-02,  1.13643803e-01, -1.04242647e-02, -7.79134855e-02,\n",
       "        2.03028740e-03,  3.89883444e-02,  3.08787189e-02,  1.45070143e-02,\n",
       "        3.85482907e-02, -6.07983358e-02, -5.43122329e-02,  6.53744861e-02,\n",
       "        3.75130512e-02, -1.50223710e-02, -3.11216656e-02, -6.78879675e-03,\n",
       "       -3.59876677e-02,  2.15049684e-02, -1.38433194e-02, -6.58503594e-03,\n",
       "        4.87125069e-02, -2.29985919e-02,  4.27762093e-03,  6.26247972e-02,\n",
       "        6.69497326e-02, -6.00897335e-02,  9.51667130e-02, -6.85305707e-03,\n",
       "       -4.09635231e-02, -2.52993200e-02, -5.15104011e-02,  2.57383520e-03,\n",
       "        2.16587959e-03,  9.52371745e-04, -5.67417294e-02,  2.17550006e-02,\n",
       "       -3.27009498e-03, -1.99578758e-02,  1.68898664e-02, -1.78895351e-02,\n",
       "       -4.21635695e-02, -6.99209496e-02,  5.29399328e-03, -5.58152869e-02,\n",
       "       -2.96953395e-02,  4.00256664e-02, -5.51384389e-02, -5.55147007e-02,\n",
       "       -1.90680288e-02, -9.32522863e-03, -4.45504524e-02,  3.63459401e-02,\n",
       "       -4.41823825e-02,  4.89771143e-02,  2.08752248e-02,  2.59078722e-02,\n",
       "        6.08277358e-02,  3.67084891e-02, -1.31767377e-01, -3.51741128e-02,\n",
       "        7.88817331e-02,  8.12838152e-02,  5.13607115e-02,  8.63512233e-02,\n",
       "       -8.24905001e-03, -4.13667001e-02,  2.42039133e-02, -9.84005863e-04,\n",
       "       -3.76848988e-02, -4.04705256e-02, -8.52179825e-02,  1.19619168e-01,\n",
       "        2.82928180e-02, -3.91437579e-03,  4.29696180e-02, -1.81124937e-02,\n",
       "       -4.02124934e-02, -1.60457324e-02, -1.73709523e-02, -4.22904454e-02,\n",
       "        4.76221442e-02, -3.73064983e-03,  2.93241674e-03, -3.85689773e-02,\n",
       "       -3.73849012e-02, -3.47426496e-02,  1.55338198e-02, -1.67733785e-02,\n",
       "        1.70088354e-02, -7.20671341e-02, -1.58908200e-02, -3.78342308e-02,\n",
       "        1.81101505e-02, -7.07381964e-02, -2.55734827e-02, -1.55217864e-03,\n",
       "        1.64393596e-02, -2.50839599e-04,  5.22818230e-02, -7.00787781e-03,\n",
       "        4.13948633e-02, -2.53964551e-02,  7.31171370e-02,  5.30831255e-02,\n",
       "       -9.55294445e-02,  3.62437181e-02, -3.07431109e-02,  3.44432741e-02,\n",
       "        1.55962193e-02, -3.67188789e-02, -6.17454946e-02, -3.49849313e-02,\n",
       "        2.72143614e-02, -2.95958482e-03, -3.37686315e-02, -7.75811449e-02,\n",
       "       -4.55394536e-02,  2.95743253e-02, -3.53848822e-02, -2.68726498e-02,\n",
       "        4.07746322e-02,  1.75229777e-02,  5.31111320e-04,  3.19801271e-02,\n",
       "        1.68966893e-02, -4.48952653e-02, -6.90863002e-03,  1.45018045e-02,\n",
       "       -1.13634560e-04,  3.59884128e-02, -1.23606119e-02, -3.30123603e-02,\n",
       "       -2.59863380e-02, -3.62134501e-02,  1.08551525e-01,  2.31313314e-02,\n",
       "       -6.62593693e-02,  4.10529338e-02, -5.61156534e-02, -2.83789281e-02,\n",
       "       -8.81508440e-02, -7.40423426e-02, -2.44430918e-02, -1.51988706e-02,\n",
       "        1.35927899e-02,  4.84278761e-02,  3.27472240e-02,  1.32557936e-02,\n",
       "       -1.88204441e-02, -6.79554492e-02,  3.93618308e-02, -7.59152025e-02,\n",
       "        6.83608092e-03, -2.02625562e-02, -9.15827900e-02, -2.76275259e-02,\n",
       "       -1.41819904e-03, -2.59369649e-02, -3.52461897e-02, -2.52892096e-02,\n",
       "        6.48952574e-02, -4.71345223e-02, -1.35595426e-02,  2.39803027e-02,\n",
       "       -5.15664741e-02, -4.69680429e-02,  1.06083322e-02, -1.02728242e-02,\n",
       "       -2.61248462e-02, -4.16483060e-02, -2.00444590e-02,  2.04669759e-02,\n",
       "        3.20508704e-02,  5.01575619e-02,  3.73929664e-02, -6.60856767e-03,\n",
       "        5.07710762e-02, -2.01186519e-02, -2.43783351e-02,  6.26228154e-02,\n",
       "       -1.23109277e-02, -2.77348962e-02, -5.98245189e-02, -6.10470660e-02,\n",
       "        4.22280021e-02,  7.02900440e-02, -4.71161939e-02, -1.96694471e-02,\n",
       "        5.46923392e-02, -3.54319625e-02, -5.26320450e-02,  7.91162020e-04,\n",
       "       -2.06545703e-02, -2.54923496e-02, -6.02235869e-02,  8.78276378e-02,\n",
       "        2.44185906e-02,  2.75565051e-02, -8.55887830e-02, -2.93072965e-02,\n",
       "        5.41291907e-02,  1.14104729e-02, -1.07414119e-01, -2.49482449e-02,\n",
       "       -3.57735492e-02,  4.55493480e-02, -3.44121680e-02,  4.16860208e-02,\n",
       "        6.15162104e-02, -2.22594719e-02,  4.40899283e-02,  5.89099526e-02,\n",
       "        1.42025519e-02, -5.14597557e-02, -1.93183403e-03, -5.90913370e-02,\n",
       "        3.63527946e-02,  4.14210372e-02,  3.03420089e-02, -1.06006404e-02,\n",
       "        6.45186240e-03, -4.68804576e-02,  1.04681678e-01,  1.47514036e-02,\n",
       "        1.19018428e-01, -6.67634048e-03,  2.90839970e-02, -1.35897502e-01,\n",
       "       -2.13052402e-03,  3.35487127e-02,  2.89693684e-03,  3.51713859e-02,\n",
       "        2.87187155e-02, -2.77524497e-02,  1.48762027e-02,  5.47290593e-02,\n",
       "        5.19304350e-02,  4.78105471e-02,  4.69145738e-02, -4.11781669e-02,\n",
       "        8.57585855e-03,  6.30197302e-03, -4.77103889e-02, -3.80771309e-02,\n",
       "       -5.43633439e-02, -2.09264681e-02, -4.74506170e-02,  3.48641016e-02,\n",
       "        4.49639596e-02,  9.19204503e-02, -5.51656075e-02, -3.52052525e-02,\n",
       "       -7.01738074e-02,  8.32224265e-03,  8.54867417e-03,  8.43890458e-02,\n",
       "        6.20543920e-02,  6.78005069e-03,  7.24386051e-02, -3.77715044e-02,\n",
       "       -6.81808293e-02, -8.59531313e-02, -6.32609576e-02,  5.72477514e-03,\n",
       "       -6.31788280e-03, -7.65029760e-03,  2.69913096e-02,  3.98124121e-02,\n",
       "        6.66562514e-03, -2.43786909e-02, -9.57071558e-02,  1.49444956e-02,\n",
       "        1.63123906e-02,  3.35621238e-02, -6.94153979e-02, -1.66427791e-02,\n",
       "       -1.04783438e-01,  3.65261920e-03, -3.98836210e-02, -3.15155722e-02,\n",
       "        4.27934621e-03, -7.88751617e-02,  3.91244665e-02, -1.25164874e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Converting entire document(review) into a numerical vector\n",
    "def document_vector(doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc.split() if word in model.index_to_key]\n",
    "    return np.mean(model[doc], axis=0)\n",
    "\n",
    "# This is the first review\n",
    "document_vector(df['review'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████| 10000/10000 [3:32:04<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# Performing the above transformation for entire corpus\n",
    "\n",
    "df = df[:10000]\n",
    "from tqdm import tqdm\n",
    "\n",
    "X = []\n",
    "\n",
    "for doc in tqdm(df['review'].values):\n",
    "    X.append(document_vector(doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting X to a numpy array\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 300)"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.03563535,  0.05202007,  0.0368176 , ..., -0.07887516,\n",
       "         0.03912447, -0.01251649],\n",
       "       [ 0.0802632 ,  0.04752473, -0.02344439, ..., -0.06213256,\n",
       "         0.04468057,  0.0056236 ],\n",
       "       [ 0.04868765,  0.07469928,  0.01612127, ..., -0.07238479,\n",
       "         0.03263976, -0.0044024 ],\n",
       "       ...,\n",
       "       [ 0.08231123,  0.0142905 ,  0.01182684, ..., -0.04006061,\n",
       "         0.02626477,  0.01509716],\n",
       "       [ 0.05773104,  0.06734917, -0.00407253, ..., -0.04985777,\n",
       "         0.03990832,  0.02976334],\n",
       "       [ 0.06943762,  0.02008892,  0.02216622, ..., -0.06904491,\n",
       "         0.03532594,  0.02165769]], dtype=float32)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-Test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2,random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8095"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X_train, y_train)\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
